
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>RandAR</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1711">
    <meta property="og:image:height" content="576">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://rand-ar.github.io/"/>
    <meta property="og:title" content="RandAR: Decoder-only Autoregressive Visual Generation in Random Orders" />
    <meta property="og:description" content="We introduce RandAR, a decoder-only visual autoregressive (AR) model capable of generatng images in arbitrary token orders. 
    Unlike previous decoder-only AR models that rely on a predefined generation order, RandAR removes this inductive bias, unlocking new capabilities in decoder-only generation. 
    Our essential design enabling random order is to insert a position instruction token before each image token to be predicted, representing the spatial location of the next image token. 
    Trained on randomly permuted token sequences -- a more challenging task than fixed-order generation, RandAR achieves comparable performance to conventional raster-order counterpart. 
    More importantly, decoder-only transformers trained from random orders acquire new capabilities. For the efficiency bottleneck of AR models, RandAR adopts parallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration without sacrificing generation quality.
    Additionally, RandAR supports inpainting, outpainting and resolution extrapolation in a zero-shot manner.
    We hope RandAR inspires new directions for decoder-only visual generation models and broadens their applications across diverse scenarios. "/>

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="RandAR: Decoder-only Autoregressive Visual Generation in Random Orders" />
    <meta name="twitter:description" content="Decoder-only Autoregressive Visual Generation in Random Orders"/>
    <meta name="twitter:image" content="" />


<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üé≤</text></svg>">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">
	<link rel="stylesheet" href="css/fontawesome.all.min.css">
	<link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

	<script defer src="js/fontawesome.all.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.5.0/Chart.min.js"></script>

    <script src="js/app.js"></script>
    <script src="js/synced_video_selector.js"></script>

</head>

<body style="padding: 1%; width: 100%">
    <div class="container-lg text-center" style="max-width: 1500px; margin: auto;" id="main">
    <!-- <div class="container" id="main"> -->
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>RandAR</b>: Decoder-only Autoregressive Visual Generation in Random Orders</br> 
            </h2>
        </div>
        <div class="row text-center">
<div class="col-md-3">
    </div>
            <div class="col-md-6 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://ziqipang.github.io/">
                            Ziqi Pang
                        </a><sup>1</sup>*
                    </li>
                    <li>
                        <a href="https://tianyuanzhang.com/">
                            Tianyuan Zhang
                        </a><sup>2</sup>*
                    </li>
                    <li>
                        <a href="https://luanfujun.com/">
                            Fujun Luan
                        </a><sup>3</sup>
                    </li>
                    <li>
                        <a href="https://yunzeman.github.io/">
                            Yunze Man
                        </a><sup>1</sup>
                    </li>
                    <wbr>
                    <li>
                        <a href="https://www.cs.unc.edu/~airsplay/">
                            Hao Tan
                        </a><sup>3</sup>
                    </li>
                    <li>
                        <a href="https://kai-46.github.io/website/">
                            Kai Zhang
                        </a><sup>3</sup>
                    </li>
                    <li>
                        <a href="https://billf.mit.edu/">
                            William T. Freeman
                        </a><sup>2</sup>
                    </li>
                    <li>
                        <a href="https://yxw.cs.illinois.edu/">
                            Yu-Xiong Wang
                        </a><sup>1</sup>
                </ul>
            </div>
<div class="col-md-3">
    </div>
            <div class="col-md-12 text-center">
                <sup>1</sup>University of Illinois Urbana-Champaign, &nbsp <sup>2</sup>Massachusetts Institute of Technology, &nbsp <sup>3</sup>Adobe Research &nbsp; &nbsp; <br>* Equal Contribution

            </div>
<br>
        <div class="row text-center">
					
			     <span class="link-block">
                <a href="https://arxiv.org/abs/2412.01827"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
                <a href="https://github.com/ziqipang/RandAR"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
			</div>

            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h3>
                        TL;DR
                    </h3>
                    <p class="text-justify">
                        We propose <b>RandAR</b>, a decoder-only AR model <b>generating image tokens in arbitrary orders</b>.
                        RandAR unlocks new capabilities for causal GPT-style transformers: parallel decoding for acceleration, inpainting, outpainting, zero-shot resolution extrapolation, and bi-directional feature encoding.
                    </p>
                </div>
            </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <img id="teaser" width="98%" src="img/teaser.png">
			</div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    We introduce <b>RandAR</b>, a decoder-only visual autoregressive (AR) model capable of generatng images in arbitrary token orders. 
                    Unlike previous decoder-only AR models that rely on a predefined generation order, RandAR removes this inductive bias, unlocking new capabilities in decoder-only generation. 
                    Our essential design enabling random order is to insert a position instruction token before each image token to be predicted, representing the spatial location of the next image token. 
                    Trained on randomly permuted token sequences -- a more challenging task than fixed-order generation, RandAR achieves comparable performance to conventional raster-order counterpart. 
    More importantly, decoder-only transformers trained from random orders acquire new capabilities. For the efficiency bottleneck of AR models, RandAR adopts parallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration without sacrificing generation quality.
    Additionally, RandAR supports inpainting, outpainting and resolution extrapolation in a zero-shot manner.
    We hope RandAR inspires new directions for decoder-only visual generation models and broadens their applications across diverse scenarios. 
                </p>
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
            <h3> <font color="#118ab2">RandAR</font>: Enabling GPT-style Transformers for Random Orders </h3>
            <p class="text-justify">
                Inspired by the success of ‚Äúnext-token prediction‚Äù in language modeling, computer vision researchers have explored using GPT-style uni-directional decoder-only transformers for image generation. 
                The typical approach tokenizes an image into discrete 2D tokens, arranges them into 1D sequences in a row-major (raster) order from top-left to bottom-right, and applies a decoder-only transformer for sequential next visual token prediction. 
                However, enforcing a uni-directional raster order limits the decoder-only transformers from modeling the bi-directional context in 2D images -- a constraint that their encoder-decoder counterparts, e.g., MaskGIT and MAR, do not face. 
                Fundamental questions thus remain: <b>Is pre-defined raster-order sequencing truly a necessary and useful inductive bias for decoder-only image generators?</b> If not, how can we equip these models with bi-directional modeling capabilities?
            </p>
            <h4> Previous Decoder-only AR Models are Limited to a Fixed (Raster) Order</h4>
                    <video id="raster" style='border: 2px solid #D9D9D9;' width="60%" muted controls>
                      <source src="videos/raster_order.mp4" type="video/mp4" />
                    </video>
                </div>
            </div>
        <br><br>
        <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h4> Our RandAR Enables Random Order via <b><font color="#118ab2">Position Instruction Tokens</font></b></h4>
                        <video id="random" style='border: 2px solid #D9D9D9;' width="100%" muted controls>
                          <source src="videos/random_order.mp4" type="video/mp4" />
                        </video>
                    </div>
                </div>
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
            <h3> <font color="#118ab2">Random Order</font> Unlocks New Capabilities for Decoder-only AR without Finetuning</h3>
            <br>
            <h4> [1] <font color="#ef486e">Parallel Decoding</font> with KV-Cache: 2.5x Acceleration </h4>
            <p class="text-justify">
                Random-order generation directly supports parallel decoding: generating multiple tokens simultaneously. This significantly accelerates AR generation by decreasing the inference steps.
            </p>
                    <video id="pd" style='border: 2px solid #D9D9D9;' width="90%" muted controls>
                      <source src="videos/parallel_decoding.mp4" type="video/mp4" />
                    </video>
                </div>
            </div>
        <div class="row">
                <div class="col-md-8 col-md-offset-2">
                <h4> [2, 3] <font color="#ef486e">Inpainting & Outpainting</font>  </h4>
                <p class="text-justify">
                    The random order ability of RandAR enables it to support image manipulation tasks of inpainting and class-conditional image editing with a uni-directional transformer. 
                    Moreover, RandAR can use full casual attention for outpainting a 256x256 image into a consistent 256x1024 one.
                </p>
                    <img id="inpainting_outpainting" width="98%" src="img/inpainting_outpainting.png">
                    </div>
                </div>
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
            <h4> [4] <font color="#ef486e">Zero-shot Resolution Extrapolation</font>  </h4>
            <p class="text-justify">
                Trained on 256x256 images, RandAR can generate 512x512 images with <b>finer details</b>, enabled by a customized hierarhical generation order. Different from previous iterative outpainting, we can generate images with 
                <b>a unified object in high-resolution details</b>.
            </p>
                <img id="resolution_extrapolation" width="98%" src="img/resolution_extrapolation.png">
                </div>
            </div>
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
            <h4> [5] <font color="#ef486e">Bi-directional Feature Encoding</font>  </h4>
            <p class="text-justify">
                By training on random orders, RandAR have acquired the ability of understanding bi-directional contexts and lead to better feature encoding, such as for finding feature correspondences.
            </p>
            <img id="bi_directional_feature_encoding" width="98%" src="img/feature_encoding.png">

                </div>
            </div>

            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                <h3> Conclusions  </h3>
                <p class="text-justify">
                    We introduce RandAR, a GPT-style causal decoder-only transformer that generates image tokens autoregressively in random orders. 
                    Our RandAR achieves this with specially designed position instruction tokens representing the location of next-token prediction. 
                    Despite the challenges of learning random order generation, RandAR achieves comparable performance with raster-order counterparts. 
                    Moreover, RandAR shows several new zero-shot applications for decoder-only models, including parallel decoding for 2.5x acceleration, inpainting, outpainting, resolution extrapolation, and feature extraction with bi-directional contexts. We hope RandAR inspires further exploration of unidirectional decoder-only models for visual tasks.
                </p>
                   
                </div>
                </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
				  <p class="text-justify" style="background-color:#D9D9D9;">
                    <textarea id="bibtex" class="form-control" style="background-color:#D9D9D9;" readonly>
@article{pang2024randar,
    title={RandAR: Decoder-only Autoregressive Visual Generation in Random Orders},
    author={Pang, Ziqi and Zhang, Tianyuan and Luan, Fujun and Man, Yunze and Tan, Hao and Zhang, Kai and Freeman, William T. and Wang, Yu-Xiong},
    journal={arXiv preprint arXiv:2412.01827},
    year={2024}
}</textarea></p>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                    <br><br>
                The website template was borrowed from <a href="http://mgharbi.com/">Micha√´l Gharbi</a>, <a href="https://dorverbin.github.io/refnerf">Ref-NeRF</a>, and <a href="https://reconfusion.github.io/">ReconFusion</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
